{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>I'm using this to share research done on my free time. Mixed with learnings from the day job and awesome people.</p> <ul> <li>I'm a Cloud Security Engineer, at least that's what the paper says.</li> <li>I like automation, almost as much as long walks on the beach.</li> </ul> <p>Find me on socials @ytimyno or Linkedin.</p> <p>P.S.: I prefer communicating in JSON and memes.</p>"},{"location":"#index","title":"Index","text":"<ul> <li>Pipelines and Protected Resources</li> <li>Weaponizing Open Source</li> <li>PaC on Metadata</li> </ul>"},{"location":"blog/devsecops/protected_resources/","title":"Pipelines and CI/CD Resources","text":"<p>With DevSecOps becoming standard, CI/CD pipelines are everywhere, building and deploying software continuously. Thousands of pipelines run daily, each executing tasks like code build, test, and deployment. Automation is key, often paired with guardrails to ensure quality and security.</p>"},{"location":"blog/devsecops/protected_resources/#tldr-access-is-everything","title":"TL;DR: Access is Everything","text":"<p>A different perspective on CI/CD security: Shifting from \"ensuring every pipeline is secure\" to \"ensuring the access to privileged resources is done under the right conditions\". </p> <ol> <li> <p>Set Outcome-Focused Expectations: Define what good looks like based on your organization's needs, risk profile, and strategic goals. User responsabilities, training, support, compliance and accountability should be clearly defined.</p> </li> <li> <p>Focus on Protected Resources: Prioritise securing access to high-impact CI/CD resources (e.g., resources that hold credentials and secrets) rather than trying to secure every pipeline individually.</p> </li> <li> <p>Lifecycle Management: CI/CD resources' lifecycle becomes the key to ensure access to these is appropriately granted and revoked. Conditions/gates must protect the resources by ensuring the right conditions are met at every use.</p> </li> <li> <p>Holistic Evaluation: When assessing CI/CD configurations, consider the platform as a whole rather than isolated settings. Ensure security and operational excellence work together. (As Microsoft puts it: \"Security recommendations are interdependent. Your posture relies on the specific recommendations you implement, which, in turn, align with your DevOps and security teams' concerns and organizational policies.\")</p> </li> <li> <p>Stay Tuned For Next Steps: Explore methods to enforce access conditions, even when native approval mechanisms may not fully support them.</p> </li> </ol>"},{"location":"blog/devsecops/protected_resources/#cicd-pipeline-basics","title":"CI/CD Pipeline Basics","text":"<p>A pipeline is a structured set of instructions that, when followed correctly, produce a desired outcome. Pipelines help ensure instructions are executed consistently and reliably, with fewer surprises.</p> <pre><code>RUN IN compute \nRUN AS user \nGET SOURCE repo\nGET VARIABLES variable_store\nDO task1\nDO task2\nDO task3\nOUTPUT logs\n</code></pre> <p>Without the resources around it, a pipeline is just a recipe in a closed recipe book - a process waiting to be executed. Ensuring the resources are accessible under the right conditions, at the time of execution, is essential.</p> <p>Semantics change across vendors, we'll focus on Azure Pipelines (concepts generally map to others).</p>"},{"location":"blog/devsecops/protected_resources/#resources","title":"Resources","text":"<p>In Azure DevOps land, CI/CD resources can take multiple forms. We can grasp how they may grant the pipelines accessing them privileges on targets.</p> <ul> <li>Agent Pools: Pipelines use agent pools to execute tasks. Sometimes agents' identities, are leveraged to access specific targets.</li> <li>Service Connections: Pipelines can authenticate and interact with external targets via service connections. They represent the access to a target/action.</li> <li>Variable Groups: These may hold shared values and sensitive information allowing pipelines access to targets.</li> <li>Secure Files: Traditionally, store sensitive files such as signing certificates.</li> <li>Repositories: Where source code is stored. At times, changes to specific repositories/branches may trigger deployment actions targeting live environments.</li> <li>Environments: Collections of resources (abstraction). Usually, production/non-production is segregated.</li> </ul> <p>Anytime a resource is mentioned in this document, it can be any of the above. </p> <p>A common theme: Everything is about access.</p>"},{"location":"blog/devsecops/protected_resources/#guardrails","title":"Guardrails","text":"<p>Guardrails give autonomy and confidence to developers executing engineering and development activities, while knowing if they are stepping outside what is deemed acceptable by the organisation. Often embedded in the CI/CD pipeline workflow, they catch non-optimal configurations at an early stage, ensuring cost-effective remediations. </p> <p>Guardrails are (semi) automated checks/controls designed to implement best practices, security standards, and compliance requirements. Guardrails help maintain development excellence while moving at an agile pace.</p>"},{"location":"blog/devsecops/protected_resources/#privileged-resources","title":"Privileged Resources","text":"<p>Resources accessed at a given time, in a given context, are, ultimately, what grants privileges to a pipeline.</p> <p>Defining the appropriate operational and security conditions for accessing a protected resource can be challenging. It's easy to disturb the balance between controls and development. This is, however, the first step into enabling the definition of a 'what good looks like' baseline. Metrics and KPIs can be derived from it. </p> <p>An example: To deploy a binary to a production repository, it must go through the vulnerability scanning process and be signed with an organisational key.</p>"},{"location":"blog/devsecops/protected_resources/#hardening-resources-access","title":"Hardening Resources Access","text":"<p>Assuming there is a defined set of operational and security conditions the pipeline must meet to access a specific resource, how can that resource be protected from misuse?</p> <p>Commonly used quality gates might provide a false sense of operational and security excellence.</p> <p></p>"},{"location":"blog/devsecops/protected_resources/#overreliance-on-weak-controls","title":"Overreliance on Weak Controls","text":"<p>Native pipeline-level controls, such as the initial access validation step, are sometimes relied upon to gate access into privileged resources. Is this good enough? Once a pipeline is allowed to access a resource (build agent, source code repository, service connection), it will persist that access indefinitely. </p>"},{"location":"blog/devsecops/protected_resources/#azure-pipelines-structure","title":"Azure Pipelines Structure","text":"<p>To understand why this is the case, we need to look at how Azure Pipelines are structured:</p> <ul> <li>Pipeline Definitions: Define the stage/job/step sequence for the CI/CD process and declare the resources it needs access to</li> <li>Pipeline Run: The execution of a pipeline, as defined by the pipeline definition</li> </ul> <p>While there are many valid reasons for updating pipeline definitions, the key point is, that once a pipeline build definition is granted access (following the initial resource access prompt), this access persists unless explicitly revoked - Regardless of any subsequent changes to the pipeline definition.</p> Make it real <p>1. Initial Pipeline Setup: \u00a0 Imagine a pipeline is created and granted specific access, such as to a sensitive build agent, source code repository, or a service connection with elevated privileges (e.g., access to deployment credentials or sensitive APIs). This access is granted based on the pipeline's initial configuration.</p> <p>2. Persistent Access: \u00a0 Once this pipeline definition has been authorised, it retains that access for all future pipeline runs, regardless of changes made to the definition later on. This means that after the first time it's authorised, Azure DevOps does not automatically revalidate permissions if the pipeline configuration is updated.</p> <p>3. Modifying the Pipeline: \u00a0 If you have access to update the pipeline definition (even as a lower-privileged user), you can modify the pipeline in a way that leverages the persistent access it has to the sensitive resources. For example, you can:  - Inject malicious or unauthorised tasks into the pipeline that use the existing service connection to exfiltrate sensitive data (e.g., source code, secrets, or deployment credentials). - Alter the pipeline to deploy malicious code or access restricted environments through the service connection. - Use the authorised build agent to execute privileged commands that were not part of the original pipeline's intent. </p> <p>4. Executing a Pipeline Run: \u00a0 After making these changes to the pipeline definition, you can trigger a new pipeline run. Since the persistent access to sensitive resources is still valid, the updated pipeline will be executed with elevated privileges.</p> <p>5. Stealth and Persistence: \u00a0 Because the access controls are applied at the pipeline definition level and are not re-validated with every change, this method allows you to continue executing malicious actions without triggering immediate alerts or permission checks. The pipeline may look legitimate, but it can be performing unauthorised actions in the background.</p> <p>See Weaponizing Open Source for a hypothetical scenario.</p>"},{"location":"blog/devsecops/protected_resources/#but-i-use-pipeline-approvals","title":"But I Use Pipeline Approvals","text":"<p>A common scenario in CI/CD is the deployment of Terraform-declared resources into a target environment. The plan -&gt; manual approval -&gt; apply pattern allows users to do a sanity check before deploying to target environments (e.g.: production). It even allows the definition of a restricted Approvers group.</p> <p>The suitability of this control depends on what you are trying to achieve. Can I skip the approval stage? By editing the pipeline definition file you can bypass guardrails, approval steps, and even inject malicious code - There are many ways for arbitrary code to find its way to execution in your run.</p> <p>How easy is it to bypass that approval step? How stealthy can a user/attacker be? How much are you relying on this standalone approval step?</p> <p>Note: There are legitimate operational reasons, with no malicious intent, users may feel the need to bypass checks.</p>"},{"location":"blog/devsecops/protected_resources/#but-i-use-branching-strategies","title":"But I Use Branching Strategies","text":"<p>That's good, however, what are you trying to achieve? Do these branching strategies alone help you get to the desired outcome? Consider whether you can create a branch from the main branch, modify the pipeline and scripts, and trigger a run with the same level of access.</p> <p>The assumption resources can only be accessed from the main branch is not always true, especially by default. You can create a branch, make changes, and execute it with the same privileges, which potentially undermines the purpose of your branching strategy.</p> <p>As Microsoft puts it: Consider prioritizing security in critical areas while accepting some trade-offs for convenience in other aspects.</p>"},{"location":"blog/devsecops/protected_resources/#protected-resources","title":"Protected Resources","text":"<p>I like to look at protected resources as a mindset. Why try to secure every single pipeline when the primary concern is the resources being accessed? Ensuring that thousands of pipeline executions align with your security expectations can be overly complex and burdensome to manage. Instead, consider securing access to privileged CI/CD resources (which those pipelines can access).</p> <p>It shifts the thinking from \"ensuring every pipeline is secure\" to \"ensuring every access to privileged resources is done under the right conditions\".</p> <p>An Azure DevOps-native way to do this is through approvals and checks. This allows you to add specific actions to be triggered anytime the deemed protected resource is accessed (from manual approvals to business hours checks, required template use, and even invoking REST APIs).</p> <p>Whether you are looking for manual approval or looking to ensure specific tools are being used within your pipelines, you define the conditions that must be met for resource access to be granted.</p> <p>An additional benefit of having a protected CI/CD resource-focused approach is, it encourages a least privileged approach where there is proper segregation of privileges (otherwise, you would constantly need to apply the highest level of scrutiny to run any pipeline). Arguably, it also increases the management effort. \"Identity is easy!\", said no one, ever.</p>"},{"location":"blog/devsecops/protected_resources/#disclaimers","title":"Disclaimers","text":""},{"location":"blog/devsecops/protected_resources/#this-is-not-new","title":"This is not new!","text":"<p>Yet I don't see heavy adoption of a protected-resource-focused approach. I wonder why that is?</p> <p>Microsoft published this table where almost all roads lead to \"Protect resource X with checks and pipeline permissions\"</p> <p>Sometimes what we uncover about the systems we work with is simply a case of RTDM (Read The Damn Manual). Refer to Microsoft's guide on protected resources for a clearer understanding.</p>"},{"location":"blog/devsecops/protected_resources/#this-is-not-just-about-security","title":"This is not just about security!","text":"<p>Protected resources are not solely about security - they are equally critical for ensuring operational excellence, sustainability, maintenance, and high-quality development processes. Balancing these factors is key to maintaining a resilient and efficient system.</p>"},{"location":"blog/devsecops/protected_resources/#this-is-also-not-a-silver-bullet","title":"This is also NOT a silver bullet!","text":"<p>The elephant in the room is that CI/CD resources are merely a means for your pipelines to access a target. Issues around access control outside the CI/CD platform are not addressed by the mechanisms we discussed.</p>"},{"location":"blog/devsecops/protected_resources/#next-challenge","title":"Next Challenge","text":"<p>Thousands of pipelines run daily, each executing tasks like code build, test, and deployment. We have all these tools and technologies that help us adhere to the best practices (guardrailing activities).</p> <p>How can we ensure all right conditions we've defined are met, even when native approvals and checks are not available?</p>"},{"location":"blog/devsecops/protected_resources/#conclusion","title":"Conclusion","text":"<p>I would focus on protecting access to service connections, variable groups and agent pools. However, if and how you leverage this piece of information is highly dependent on your environment and strategy.</p> <p>This is the first of several upcoming research findings on CI/CD, with more insights and analysis. Stay tuned for deeper dives!</p>"},{"location":"blog/devsecops/protected_resources/#references","title":"References","text":"<ul> <li>OWASP's Top 10 CI/CD Security Risks</li> <li>Microsoft's Azure Pipelines Security Resources</li> <li>Cloud Security Alliance: DevSecOps Pillars</li> <li>Daniel Abrahamberg's Enterprise Development: Guidelines, Guardrails, and Golden Paths</li> </ul>"},{"location":"blog/technical/py_git_revshell/","title":"Weaponizing Open Source","text":"<p>Often, the risks and complexities of running unvetted code - especially in the open-source world - are overlooked or underestimated. This research highlights the potential consequences of blindly integrating external code into your system, emphasizing the importance of Supply Chain Security.</p> <p>Scenario: You have a goal. You need to ensure the resources in your development pipelines are being configured according to your policy. You find a GitHub project that does exactly what you need. This project is a fork of the widely used Checkov project, which you already use anyway, but adds some useful custom checks.</p> <p>We explore how the open source tool Checkov, along with Python and Git, can be combined to exfiltrate data covertly. Particularly relevant after the discovery of the XZ Utils backdoor. </p> <p>Disclaimer</p> <p>Open source is incredible. The contributions from open source developers, who voluntarily maintain widely used projects, provide immense benefits to the community. This post is not intended to detract from the value of their work, but rather to emphasise the importance of understanding the potential risks involved when integrating external code into your system.</p>"},{"location":"blog/technical/py_git_revshell/#checkov-101","title":"Checkov 101","text":"<p>\"Checkov is a static code analysis tool for scanning infrastructure as code (IaC) files for misconfigurations that may lead to security or compliance problems. Checkov includes more than 750 predefined policies to check for common misconfiguration issues. Checkov also supports the creation and contribution of custom policies.\"</p> <p>Checkov is often integrated into build pipelines (in a CI/CD system) and can also be run locally. It is packed as a command line tool that runs a series of checks against the resources targeted by the scan. Example:</p> <p>Focusing on what happens when we are specifying extra custom checks in Checkov, we can either:</p> <ol> <li>Specify a directory with the extra checks' code:</li> </ol> <pre><code> checkov --directory *path/to/resources* --external-checks-dir *path/to/custom/checks*\n</code></pre> <ol> <li>Specify a Git repo that contains the extra checks' code.</li> </ol> <pre><code>checkov --directory *path/to/resources* --external-checks-git *customchecks_giturl*\n</code></pre> <p>Whichever the way chosen, Checkov checks are really just running code, and Python-based custom checks, are really just running custom Python code.</p> <p>Going back to the original scenario. You are almost achieving your goal of ensuring that all your resources are configured according to custom checks, and this is enabled by the (free!) custom extra checks you found. Great win. What can go wrong?</p>"},{"location":"blog/technical/py_git_revshell/#data-exfiltration-with-python-and-git","title":"Data Exfiltration with Python and Git","text":"<p>When running Checkov checks (custom or not), you are running code, in your environment, with all the permissions the user runinng it has. This is a great foothold into your environment.</p> <ul> <li>We have (almost arbitrary) command execution with Python</li> <li>We can leverage tools such as Git to in/exfiltrate data</li> </ul> <p>To illustrate this, we have exploit code for:</p> <ul> <li> <p>A proof of concept CKV_COOL_MALI_CHECK that creates a user, generates files, runs bash commands, and installs an OpenSSH server on the machine running Checkov. This allows remote connections using the new user. While this requires sudo privileges and firewall rules might complicate access, the potential for misuse is significant, especially regarding filesystem integrity.</p> </li> <li> <p>A proof of concept CKV_COOL_MALI_2_CHECK to silently exfiltrate data with Python and Git:</p> </li> <li> <p>Clones a remote repository (git clone)</p> </li> <li>Creates a new branch* (git checkout)</li> <li>Creates a file with data about the system in the cloned repo directory (keep it simple for PoC, endless opportunities)</li> <li>Commits file to the .git tree (git add &amp; commit)</li> <li>Pushes to the Git repo (git push)</li> <li>Deletes traces of its activity (shutil.rmtree)</li> </ul> <p></p>"},{"location":"blog/technical/py_git_revshell/#conclusion","title":"Conclusion","text":"<p>The sky is the limit here. With or without root privileges, you can leave a mark. My advice is not to run extra checks unless the code has been reviewed and tested by yourself (you do have to do the work, the \"it's fine, open source means it's safe because everyone can check it\" solution has been proven NOT to work).</p> <p>This caution applies to any tool where the source code has not been scrutinized. It's not just Checkov, nor is it just Checkov custom checks.</p>"},{"location":"blog/technical/py_git_revshell/#actions","title":"Actions","text":"<ul> <li> <p>Reached out to Palo Alto through their security reports page.</p> </li> <li> <p>Checkov documentation has been updated.</p> </li> </ul>"},{"location":"blog/technical/py_git_revshell/#related-work-stories","title":"Related Work &amp; Stories","text":"<p>This research was triggered when developing custom policies to help with metadata guardrails for container images and CSP infrastructure (labels, tags, annotations, etc). Metadata becomes more complex than expected when you consider the semantics used by different providers, the asset types and the lack of maturity of tools in this space. A starter-for-ten is available on pac-on-rails.</p>"},{"location":"blog/technical/pac/pac_on_rails/","title":"pac-on-rails","text":"<p>The code, tests and research are available in the public repository pac-on-rails, feel free to have a look and tailor to your needs. In here for completeness.</p> <p>Mini project to automate Policy as Code (PaC) and guardrails to ensure adherence to tagging/labelling policies. Supports Traditional IaC Infrastructure and Container Images.</p>"},{"location":"blog/technical/pac/pac_on_rails/#policy-engine","title":"Policy Engine","text":"<ul> <li> <p>The repository holds Checkov custom policies to perform checks against container images' labelling policies. The engine ensures compliance with the specified policy (see customisation section).</p> </li> <li> <p>The repository holds Checkov custom policies to perform checks against infrastructure metadata policies (Terraform-defined). The engine ensures compliance with the specified policy (see customisation section).</p> </li> </ul>"},{"location":"blog/technical/pac/pac_on_rails/#metadata-policy-format-container-images-traditional-iac-infrastructure","title":"Metadata Policy Format (Container Images &amp; Traditional IaC Infrastructure)","text":"<p>The tagging policy is defined using JSON format, allowing for flexible customisation of metadata pairs and their validation rules.</p> <pre><code>{\n    \"maintainer\": {\n        \"allowed_values\": \".*\",\n        \"version\": \"1.0\",\n        \"description\": \"A sample metadata pair - Any value accepted\"\n    },\n    \"maintainer_specific\": {\n        \"allowed_values\": \"^[\\w\\.-]+@[a-zA-Z0-9-]+\\.[a-zA-Z]{2,}$\",\n        \"version\": \"1.0\",\n        \"description\": \"A sample metadata pair - Specific regex\"\n    }\n}\n</code></pre>"},{"location":"blog/technical/pac/pac_on_rails/#extendable-csp-specific-configuration-traditional-iac-infrastructure","title":"Extendable CSP-specific Configuration (Traditional IaC Infrastructure)","text":"<p>Each cloud provider has its own configuration section within the engine. The JSON file lists specific configurations to take, according to the CSP. This is here, not only, but also because CSPs use different terms to refer to the same thing.</p> <p>This policy is currently applicable to Terraform resources. As you will find out, different providers, and even different resources within the same provider, use different attributes and attribute \"paths\" to apply metadata at different scopes. </p> <pre><code>{\n  \"azure\": {\n      \"keys\": [\n          \"arm\",\n          \"az\",\n          \"azure\",\n          \"azurerm\"\n      ],\n      \"description\": \"Resources to check for metadata pairs (Azure). To override this, modify this file, leaving it in the working directory checkov runs from.\",\n      \"supported_types\": [\n          {\n              \"name\": \"azurerm_kubernetes_cluster\",\n              \"tag_paths\": [\n                  {\n                      \"path\": \"\",\n                      \"attributes\": {\n                          \"one_of\": [],\n                          \"required\": [\n                              {\n                                  \"name\": \"tags\",\n                                  \"cloud_native\": false\n                              }\n                          ]\n                      }\n                  },\n                  {\n                      \"path\": \"default_node_pool\",\n                      \"attributes\": {\n                          \"one_of\": [],\n                          \"required\": [\n                              {\n                                  \"name\": \"tags\",\n                                  \"cloud_native\": false\n                              }\n                          ]\n                      }\n                  }\n              ]\n          }\n      ]\n  }\n}\n</code></pre> <ul> <li><code>csp</code>: Configuration related to the CSP resources. Contains details on how to validate tags for that CSP infrastructure.</li> <li> <p>Example Values: <code>azure</code>, <code>google</code>, <code>aws</code></p> </li> <li> <p><code>keys</code>: List of identifiers used to recognize CSP-related resources. These keywords help in identifying and applying the tagging policy to the correct resource types.</p> </li> <li> <p>Example Values: <code>[\"arm\", \"az\", \"azure\", \"azurerm\"]</code> or <code>[\"gcp\", \"google\", \"googlecloud\"]</code></p> </li> <li> <p><code>supported_types</code>: List of CSP resource types targeted by the tagging policy. Each type includes specific tag paths and requirements.</p> </li> <li>Example Values:<ul> <li><code>name</code>: The name of the resource type (e.g., <code>azurerm_kubernetes_cluster</code>).</li> <li><code>tag_paths</code>: List of paths within the resource where tags are validated.</li> <li><code>path</code>: Optional sub path eithin the resource configuration, leads to the tags attribute (e.g., <code>default_node_pool</code>).</li> <li><code>attributes</code>:<ul> <li><code>one_of</code>: At least one of these must be present (e.g., <code>tags</code>).</li> <li><code>required</code>: All these attributes must be present (e.g., <code>tags</code>).</li> </ul> </li> </ul> </li> </ul>"},{"location":"blog/technical/pac/pac_on_rails/#customise","title":"Customise","text":"<p>Customise with cloud_specific_configurations.json and policy.json as needed. Else, it will use a pre-def set of values. To customise a file, it needs to be in the dir where checkov runs from (see future work).</p>"},{"location":"blog/technical/pac/pac_on_rails/#security-advisory-before-running","title":"Security Advisory - Before Running","text":"<p>To use extra checks with Checkov CLI tool, we can:</p> <ol> <li>Specifying a directory with the extra checks' code, or</li> <li>Specifying a Git repo that contains the extra checks' code.</li> </ol> <p>I'd recommend (1). And if you ask me about number (2), I'd ensure the checks are from a private and/or very well scrutinised git repo. </p> <p>Checkov checks are really just running extra code:</p> <ul> <li>With custom Python checks, it's running custom Python scripts. </li> <li>With custom Python checks from a remote repo, it's running (potentially dangerous) custom Python scripts from a remote repo. </li> </ul> <p>Sky is the limit here. With or without root privileges, you can leave a mark. My advice is not to run extra checks unless the code has been reviewed and tested by yourself (you do have to do the work, the \"it's fine, open source means it's safe because everyone can check it\" solution has been proven NOT to work). This is true with any (especially open source) tool. </p> <p>That said, I feel the official Checkov documentation and GitHub pages could be clearer. The only place I could find that lightly touches on this is a small paragraph on best practices in the GitHub page (https://github.com/bridgecrewio/checkov?tab=readme-ov-file#configuration-using-a-config-file).</p> <p>Update: Reached out to Palo Alto through their security reports page. Checkov documentation has been updated.</p>"},{"location":"blog/technical/pac/pac_on_rails/#malicious-check-1-user-ssh","title":"Malicious Check 1 - User + SSH","text":"<p>I done a simple PoC that creates a user, creates a files and runs bash commands. It also installs an openssh server in the machine running Checkov which we can connect to it, using the created user. Installs and user creation requires sudo priviledges, and connections may be harder with FW rules in place, but possibilities are endless, and I have seen loads of scary attacks on workloads. Particularly scary on the integrity side, as we can mess with the filesystem.</p> <pre><code>1. Try to SSH (not running, so I have connection refused)\n2. Run malicious check from remote git repo\n3. SSH with the newly created user\n</code></pre>"},{"location":"blog/technical/pac/pac_on_rails/#malicious-check-2-infiltrate-exfiltrate-data","title":"Malicious Check 2 - Infiltrate + Exfiltrate data","text":"<p>Take a step back, what would an attacker want?</p> <ul> <li>We have (almost arbitrary) command execution </li> <li>I guess we need a way to infiltrate/exfiltrate data!</li> </ul> <p>Queue CKV_COOL_MALI_2_CHECK. This check silently:</p> <ol> <li>Clones a remote repository (git clone)</li> <li>Creates a new branch* (git checkout)</li> <li>Creates a file with data about the system in the cloned repo directory (keep it simple for PoC, endless opportunities)</li> <li>Commits file to the .git tree (git add &amp; commit)</li> <li>Pushes to the Git repo (git push)</li> <li>Deletes traces of its activity (shutil.rmtree)</li> </ol> <p>*(because our attacker adheres to best practices and does not push to main ;) )</p> <p>I wanted some kind of reverse shell, but was being slowed by some connection refused errors.</p> <p></p>"},{"location":"blog/technical/pac/pac_on_rails/#run","title":"Run","text":""},{"location":"blog/technical/pac/pac_on_rails/#container-images","title":"Container Images","text":"<p>checkov -d containerfile_dir/ --external-checks-dir checks/metadata/images --framework dockerfile -c CKV_DOCKER_LABEL_CHECK</p>"},{"location":"blog/technical/pac/pac_on_rails/#traditional-iac-infrastructure","title":"Traditional IaC Infrastructure","text":"<p>checkov -d terraform_dir/ --external-checks-dir checks/metadata/infrastructure -c CKV_TF_METADATA_CHECK </p>"},{"location":"blog/technical/pac/pac_on_rails/#future-work","title":"Future Work","text":"<ul> <li>Combine Checks: Explore combining Cloud-Native (CN) and traditional IaC checks into a unified metadata validation approach.</li> <li>Customize Input Paths: Customize the path to input files (policy.json, cloud_specific_configurations.json) for higher flexibility.</li> </ul>"},{"location":"blog/technical/pac/pac_on_rails/#feedback-contact","title":"Feedback &amp; Contact","text":"<p>For questions, feedback, bug reports, please use one of the below:</p> <ol> <li> <p>GitHub Issues: Report bugs or request features on GitHub Issues page.</p> </li> <li> <p>Email: For general feedback, you can email at ytimyno@gmail.com.</p> </li> </ol>"}]}