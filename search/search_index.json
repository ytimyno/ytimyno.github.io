{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>I'm using this to share research done on my free time. Mixed with learnings from the day job and awesome people.</p> <ul> <li>I'm a Cloud Security Engineer.</li> <li>I like automation and JSON.</li> </ul> <p>Find me on socials @ytimyno or Linkedin.</p>"},{"location":"#research","title":"Research","text":"<ul> <li>Resources &amp; Pipelines</li> <li>Resource Lifecycle</li> <li>Build Servers</li> </ul>"},{"location":"#playground","title":"Playground","text":"<ul> <li>From open source package to credential exfiltration from your build environment</li> <li>Weaponizing Open Source</li> <li>PaC on Metadata</li> </ul>"},{"location":"blog/cicd/access_lifecycle/","title":"CI/CD Resource Lifecycle - Identity Proxy","text":"<p>The first post in the series looks at Pipelines and CI/CD Resources. A pipeline boils down to a structured set of instructions and its access to (CI/CD) resources. Whereby these CI/CD Resources serve as a link to an underlying identity.</p> <p>Previously, I proposed setting an outcome-focused goal. Shifting from \"ensuring every pipeline is secure\" to \"ensuring the access to CI/CD Resources is done under the right conditions\". This constitutes multiple steps:</p> <ol> <li> <p>Defining what are these right conditions;</p> </li> <li> <p>Defining which CI/CD Resources are privileged;</p> </li> <li> <p>Managing CI/CD Resources' lifecycle.</p> </li> </ol> <p>We will start addressing (3): Managing CI/CD Resources' lifecycle.</p> <p></p>"},{"location":"blog/cicd/access_lifecycle/#identity-proxy","title":"Identity Proxy","text":"<p>To understand what a CI/CD Resource lifecycle solution could look like, including what it addresses and potential gaps, the identity proxy concept is useful.</p> <p>Identities are unique and, by nature, cannot be stolen or shared. Credentials are our attempt to represent this uniqueness in the digital world. In this sense, credentials serve as an identity proxy - a way to turn a real-world identity into a usable digital format.</p> <p>This section is inspired by Identity-Native Infrastructure Access Management by Ev Kontsevoy, Sakshyam Shah, Peter Conrad. The authors discuss how credentials challenge the true nature of identity. While an identity itself is immutable and non-transferable, credentials attempt to represent it as data - something that can be shared (and even spoofed). The book explores potential solutions for these challenges. </p>"},{"location":"blog/cicd/access_lifecycle/#generic-identity-proxy-model","title":"Generic Identity Proxy Model","text":"<p>Introducing the concept of an identity proxy opens up a path that encourages us to look beyond platform-specific controls - I recognise my Azure DevOps bias! By doing so, we can develop a more generic model, offering long-term benefits and adaptability, rather than being confined to ADO-specific approaches.</p> <p>This enables us to envision a platform-agnostic solution that meets our security and operational efficiency expectations.</p> <p></p> <p>In this case, the credential is the CI/CD Resource. I will nevertheless, continue with an ADO-specific analysis, as it allows for a quick(er) win.</p>"},{"location":"blog/cicd/access_lifecycle/#ado-identity-proxy-model","title":"ADO Identity Proxy Model","text":"<p>Azure DevOps (ADO) natively segregates the pipeline from the CI/CD Resources (via service connections, agent pools, variable groups, secure files, etc). </p> <p>ADO CI/CD Resources (like those used in a pipeline) often stand as a secondary representation of a credential. And, as we've seen, credentials are considered an identity proxy.  </p> <p></p> <p>In Azure DevOps, CI/CD Resources come in various forms. Ultimately, each type serves as a proxy for a credential, thus, an indirect link to an identity.</p> Example: Azure DevOps CI/CD Resources as Proxies on Identity Proxies <p>1. Agent Pools: Agents, where the CI/CD processes run, may have access to credentials (digital certificates, SSH keys, and other credential types). Access to an agent, a CI/CD Resource, means access to the credentials it has access to, resulting in access to the identities those credentials proxy. Authentication and authorisation to services is done based on those credentials.</p> <p>2. Service Connections: Service connections allow pipelines to interact with external services by using stored credentials (like tokens or API keys). Access to a service connection, a CI/CD Resource, means access to the credentials it stores, resulting in access to the identities those credentials proxy. Authentication and authorisation to services is done based on those credentials.</p> <p>3. Variable Groups: Variable groups hold sensitive values (e.g., passwords, keys) that pipelines use without direct access to the raw credentials. Access to a variable group, a CI/CD Resource, means access to the credentials it stores, resulting in access to the identities those credentials proxy. Authentication and authorisation to services is done based on those credentials.</p> <p>4. Secure Files: Secure files, such as certificates or keys, are referenced in pipelines without exposing their contents. Access to a secure file, a CI/CD Resource, means access to the credentials it stores, resulting in access to the identities those credentials proxy. Authentication and authorisation to services is done based on those credentials.</p> <p>5. Repositories: Repositories are not as straightforward. One scenario to imagine is, if a GitOps process uses a repository as a source of truth, it means there is an external process monitoring the contents of that repository. That external process may have its own access to ensure the state between the repository and the runtime is synced. In this scenario, access to the repository could be seen, indirectly, as providing access to credentials used by the external processes.</p> <p>6. Environments: \"An environment is a grouping of resources where the resources themselves represent actual deployment targets\" - Accessing the environment grants access to service connections and secure files that have been configured to access those targets.</p>"},{"location":"blog/cicd/access_lifecycle/#solution-threat-model-mini","title":"Solution Threat Model (mini)","text":"<p>It's at the credential level that we can mediate access. However, each credential/proxy added to the solution is an additional element that can be misconfigured or exploited. Consider a scenario where a second (equivalent) credential is created and accessible to the pipeline, this could mean any conditions of access previously established are bypassed.</p> <p></p> <p>Moreover, looking at the Azure DevOps Identity Proxy Model, we can see the concern is multiplied. Imagine a scenario where:</p> <ul> <li> <p>A critical workload is intended to be managed via CI/CD processes and automation.</p> </li> <li> <p>A service connection is created. This service connection stores the credentials to access the critical workload. </p> </li> <li> <p>Access to the service connection is protected by a robust set of conditions, ensuring approvals are required before its use.</p> </li> </ul> <p>There are more questions we need to ask ourselves when adopting this model, more things we need to ensure are under control (diagram helps visualising these four questions):</p> <ol> <li> <p>Can someone update the service connection, removing the robust set of conditions applied?</p> </li> <li> <p>Can someone create a rogue \"twin\" service connection, pointing at the same credentials, removing the robust set of conditions previously applied?</p> </li> <li> <p>Can someone create a rogue service connection that points at rogue credentials, removing the robust set of conditions previously applied?</p> </li> <li> <p>Can someone update the service connection to point at some other credential for some other identity?</p> </li> </ol> <p></p>"},{"location":"blog/cicd/access_lifecycle/#is-the-extra-complexity-in-the-ado-identity-proxy-model-a-good-thing","title":"Is the extra complexity in the ADO Identity Proxy Model a good thing?","text":"<p>As with any architecture that starts having too many arrows and dependencies, concerns start arising. Implementing a system that ensures consistent identity representation across all layers becomes increasingly difficult.</p> <p>However, what is distinctive about this model is, it allows us to implement those conditions of access. In fact, ADO, by separating pipelines from CI/CD Resources, provides one of the most effective methods I've seen for mediating access. This separation allows for the application of conditions and access controls that are specifically based on the access needs themselves, leading to a more adaptable framework.</p>"},{"location":"blog/cicd/access_lifecycle/#identity-proxy-conclusion","title":"Identity Proxy Conclusion","text":"<p>The best approach seems to be addressing identity as closely as possible, which is the idea behind the Generic Identity Proxy Model. Currently, I'm not aware of existing solutions that fully support this concept. Therefore, let's focus on what we can implement today while gathering insights that inform longer-term strategies.</p>"},{"location":"blog/cicd/access_lifecycle/#lifecycle","title":"Lifecycle","text":"<p>Before implementing lifecycle practices, it's essential to have clear definitions of the people, processes, and technology required to access a given CI/CD Resource. There are multiple challenges associated with identifying the right criteria for access and implementing it (which I plan to address on a later date, and introduce here).</p>"},{"location":"blog/cicd/access_lifecycle/#the-lifecycle-of-a-cicd-resource","title":"The Lifecycle of a CI/CD Resource","text":"<p>The lifecycle of a CI/CD Resource focuses on ensuring its protection is in line with expectations and policies. Effective management encompasses the entire lifecycle of the resource through creation, reading, updating, and deletion (CRUD), ensuring that:</p> <ul> <li> <p>Policy Application: When a resource is created, a set of policies is applied to control access to the CI/CD Resource.</p> </li> <li> <p>Access Control: The expected entities (i.e.: pipeline) can read (or use) the resource if the defined policies are met.</p> </li> <li> <p>Authorisation: Creating, updating, reading, and deleting of resources is restricted to authorised entities, limiting what actions the entities may perform, including from where and when they can be performed.</p> </li> <li> <p>Auditability: Changes, access, creation, and deletion are auditable and guarantee non-repudiation.</p> </li> <li> <p>Monitoring and Alerts: Monitoring and alerts are established based on the criticality and sensitivity of the resource. Alerts are generated and addressed for unexpected actions on CI/CD Resources.</p> </li> <li> <p>Stale Resource Management: Stale resources are identified and deleted.</p> </li> </ul> <p>If using Azure DevOps, the high-level approach can be summarised as the continuous:</p> <ol> <li> <p>Define and review what good looks like by establishing the conditions under which resources should be accessed.</p> </li> <li> <p>Discover and identify the most essential CI/CD Resources for your operations.</p> </li> <li> <p>Apply the necessary approvals and checks (Azure DevOps protected resource conditions) that align with the conditions defined for the CI/CD Resources in scope.</p> </li> </ol>"},{"location":"blog/cicd/access_lifecycle/#disclaimers","title":"Disclaimers","text":""},{"location":"blog/cicd/access_lifecycle/#identity-aware-proxies","title":"Identity Aware Proxies","text":"<p>I'm beginning to see the potential for extending and integrating this narrative beyond just CI/CD pipelines into other systems and platforms. This aligns with the concept of identity-aware proxies, where we can draw parallels to solutions like Google's Identity-Aware Proxy (IAP) and Microsoft Entra ID's Conditional Access. We've observed the introduction of Conditional Access for workload identities, which suggests this could be a direction for us to explore. I plan to investigate this further, as it's a promising concept that resonates with me!</p>"},{"location":"blog/cicd/access_lifecycle/#easier-said-than-done","title":"Easier Said Than Done","text":"<p>Most organisations do not operate in a greenfield environment. Implementing certain checks may create ripples in existing processes. However, it's important to recognise that currently, your CI/CD resources may not be as protected as expected.</p>"},{"location":"blog/cicd/access_lifecycle/#implementing-policy-which-policies-approvals-and-checks-should-you-use-and-how","title":"Implementing Policy: Which policies, approvals and checks should you use? And how?","text":"<p>Note there are multiple (ADO-native) ways to protect CI/CD Resources. However, certain protections may require heavy refactoring of existing processes (e.g., extending templates), which might not be desirable.</p>"},{"location":"blog/cicd/access_lifecycle/#crawl-walk-run-stop-the-rot-address-legacy-cicd-resources","title":"Crawl, Walk, Run: Stop the rot + Address legacy CI/CD Resources","text":"<p>You will probably need a deployment plan. It might look something like this, and it might happen in parallel with the policy definition.</p> <ol> <li> <p>Establish a rule whereby any new CI/CD Resources created must undergo the defined lifecycle process to ensure compliance with the policies and protections defined.</p> </li> <li> <p>Start understanding your service connections, agent pools, and variable groups, including their purposes. This assessment will likely reveal overprivileged resources (related to the principle of least privilege and separation of duties). </p> </li> <li> <p>Identify the types of conditions and protections that best suit your CI/CD Resource environment, balancing benefits, complexity, and desired outcomes. Perhaps above all, the choice between options offered by ADO will largely depend on your organisation's maturity and agility for change.</p> </li> </ol>"},{"location":"blog/cicd/access_lifecycle/#next-challenge","title":"Next Challenge","text":"<p>Assuming what good looks like is defined in the context of using each CI/CD resource you care about, how can we ensure those policies are adhered to? We can use a combination of soft and hard controls, across People, Process and Technology. Can we make it easy for users to leverage the available toolkits and meet our expectations? How can we seamlessly integrate technology into processes, allowing people to focus on following them? </p>"},{"location":"blog/cicd/access_lifecycle/#conclusion","title":"Conclusion","text":"<p>One of the key takeaways from our previous discussion is that access is important when managing pipelines. To transition from a focus on \"ensuring every pipeline is secure\" to \"ensuring access to CI/CD Resources is granted under the right conditions\", we must implement effective identity (or identity proxy) lifecycle management.</p> <p>The concepts we've explored have allowed us to begin addressing how we can meet all the right conditions we've established, even in the absence of native approvals and checks. Though the solution proposed builds on Azure DevOps native concepts, it remains adaptable for future integration with strategic, vendor-neutral, solutions.</p>"},{"location":"blog/cicd/access_lifecycle/#references","title":"References","text":"<ul> <li> <p>Identity-Native Infrastructure Access Management by Ev Kontsevoy, Sakshyam Shah, Peter Conrad</p> </li> <li> <p>Threat Modeling by Izar Tarandach, Matthew J. Coles</p> </li> </ul>"},{"location":"blog/cicd/execution_agents/","title":"CI/CD Build Servers","text":"<p>There's more to a CI/CD platform than these recipes we call \"pipelines\". These recipes are instruction sets that define workflows and which resources are accessed. But these workflows need to run somewhere.</p> <p>CI/CD resources declared in pipelines can be source code repositories, files, pre-configured credentials or vaults, etc. There is a special kind of resource - Build Servers, where the pipeline is scheduled and executed.</p> <p>Mental Note 1: Whether using private, cloud-hosted or SaaS-managed machines, there is no magic, the steps of a CI/CD workflow must be executed in some sort of compute</p> <p></p>"},{"location":"blog/cicd/execution_agents/#what-does-the-work","title":"What does the work","text":"<p>Diagrams can sometimes oversimplify details that mask the workings of a system in not so obvious ways. I've fallen into this trap before. It's easy to accept the status quo which seems to follow the SDLC overarching model of Design -&gt; Code -&gt; Build -&gt; Deploy -&gt; Run.</p> <p>Typical (classic) security controls, such as Endpoint Detection and Response (EDR), Data Loss Prevention (DLP), vulnerability scanning, etc, are largely applied to compute in the far ends of this process: User endpoints and running workloads. It's easy to forget what goes on in between.</p> <p></p> <p>We shall challenge this view!</p>"},{"location":"blog/cicd/execution_agents/#pipelines-are-code-execution-on-the-left","title":"Pipelines are code execution \"on the left\"","text":"<p>Working with CI/CD pipelines for a while, we can start to see how a pipeline execution on a build agent should be treated as runtime, just like any other. With a few gotchas:</p> <ul> <li>Gotcha 1: Build and deploy environments have direct access to the environments you are deploying to</li> <li>Gotcha 2: Build and deploy environments are complex, with plenty of layers that make detection and attribution hard</li> <li>Gotcha 3: Build and deploy environments are less scrutinized and their criticality is often underestimated, compared to classic application or enterprise runtimes.</li> </ul> <p></p> <p>Here is how this combination of gotchas starts to make me twitch.</p>"},{"location":"blog/cicd/execution_agents/#collect-identities","title":"Collect Identities","text":"<p>CI/CD workflow activities involve interacting with resources, artifacts, workloads, data, services. Authentication as an identity needs to happen, and you'll need to have the right authorization in place. </p> <p>At some point, all the credentials that allow you to impersonate identities authorized to access resources will be accessible from the build server. </p> <p>There is no magic behind it. The secrets, the SSH private key files, the tokens, the 'secretless authentication' files required to access a target, will all be where they can be used to perform the sequence of instructions (steps) that have been passed onto the agent.</p> <p>What makes this even more involved are the different identity layers:</p> <ul> <li>Pipeline identity</li> <li>Identities to access the CI/CD Platform (User/Service, whether local accounts, SaaS-managed or federated)</li> <li>Identities to access the target runtime workloads and services</li> </ul> <p>Against different system components:</p> <ul> <li>CI/CD Platforms themselves (platform settings, resources hosted in the platform itself - i.e.: CI/CD platform settings around CI/CD resource and pipeline permissions and access control, such as repositories, credentials, build servers)</li> <li>Execution environment (who can access the agent executing the workflow, with what level of privilege - i.e.: having direct access to the host running CI/CD workflows with superuser privileges)</li> <li>Runtime workloads (who/what can access the runtime workloads, how can they affect them, directly or indirectly - i.e.: the credentials used to access targets represent a specific level of privilege)</li> </ul> <p>A picture is worth a thousand words. Note the different kinds of identities (or resources) that go through the build server. Each will have (directly or indirectly) a set of capabilities on target resources.</p> <p></p> <p>Because identity is everywhere, this quickly escalates to an \"unmanageable mess\". Ultimately, without access, you're limited in what you can do in the environment. Humans be humans, and overtime the tendency is to converge into the path of least resistance.</p> <p>Now is probably a good time to do some introspection on how robust your CI/CD access management is, across users, service accounts and pipelines, against the different system components. Because it'll only get harder.</p>"},{"location":"blog/cicd/execution_agents/#scenarios-self-hosted-agents-in-azure-devops","title":"Scenarios - Self-Hosted Agents in Azure DevOps","text":"<p>I played and recommend playing with self-hosted CI/CD build servers to really grasp their power. If using Azure DevOps, the article A Security Analysis of Azure DevOps Job Execution is detailed enough to understand the job execution workflow.</p> <p>It's cool to play, but the so what question comes along. How would someone even get onto the machine, especially when in an organization that has multiple layers of defense. Direct access to the build agent is needed - or is it? (- that was sarcastic, it's not needed).</p> Access Vectors Impact Insider threatCompromised user accountMalicious or vulnerable binary/package (supply chain) Data/secret exfiltrationDenial of Service/WalletPersistence/Lateral movementCompromise of internal artifactsStop/tamper with security tooling <p>Once you have access to the build agent through any of the access vectors above - chaos!</p>"},{"location":"blog/cicd/execution_agents/#insider-threat-or-insider-account-compromise","title":"Insider Threat or Insider Account Compromise","text":"<p>Knowing a the typical developer/engineer needs (and has) access to run CI/CD workflows, this gives them access to build servers (agents). In this scenario, there is a valid user account that can access a build agent. </p> <p>We are not solely speaking via the traditional SSH connection into the build server machine/VM. Rather, access to the build server VM is a by-product of scheduling a pipeline.</p> <p>\"Tasks run in an execution context, which is either the agent host or a container.\"</p> <p>Microsoft Learn - target definition</p> <p>This YAML snippet is telling the Azure Pipelines service to run the job directly on the target host. It is the default mode.</p> <pre><code>jobs:\n- job: RunOnTargetHost\n  steps:\n  - script: echo \"Running job on target host\"\n</code></pre> <p>This YAML snippet is telling the Azure Pipelines service to run the job in a container on the host.</p> <pre><code>jobs:\n- job: RunOnTargetContainer\n  container: python\n  steps:\n  - script: echo \"Running in job container\"\n</code></pre> <p>This YAML snippet is telling the Azure Pipelines service to run the job in a container on the host, with the second step targeting the host.</p> <pre><code>jobs:\n- job: RunOnTargetContainer\n  container: python\n  steps:\n  - script: echo \"Running in job container\"\n  - script: echo \"Running job on target host\"\n    target: host\n</code></pre> <p>This means, if we want access to the build server host VM, all we need to do is not specify a container to run the job on or explicitly setting the target as being the host. Additionally, when running within a container, we're likely to be able to access the host via the Docker socket running in <code>/var/run/docker.sock</code>.</p> <p>Ability to influence the pipeline configuration/definition means you can influence what/where/when it gets executed. And there are many ways to influence how a pipeline runs its course (some more obvious than others).</p> <p>Mental Note 2: Access to the host is easily obtained (directly or indirectly)</p>"},{"location":"blog/cicd/execution_agents/#theres-more-whoami","title":"Theres more - $whoami","text":"<p>Guess what! Using \"Virtual Machine Scale Set\" option for you Azure DevOps build agent pool comes with default configuration scripts. And guess what users you get by default?</p> <p>No, you're not root. That would go against basic Linux best practices! Instead, if you're following the standard set up, you will have an AzDevOps user account which happens to be able to run any command as any user with sudo without being prompted for a password.</p> <p></p> <p>Mental Note 3: Likely the identity used to run the pipeline workflow process has superuser privileges on the build server</p> <p>Do note this is easy to complain about, and harder to fix. We have to keep in mind security is not the end goal with this systems. That said, try to at least have compensating controls.</p>"},{"location":"blog/cicd/execution_agents/#beyond-insider-threat-and-compromised-accounts-open-source-library-supply-chain-compromise-scenario","title":"Beyond Insider Threat and Compromised Accounts: Open Source Library Supply Chain Compromise Scenario","text":"<p>Insider threats and compromised accounts aren't the only ways to get access to the build environment. </p> <p>The risks and complexities of running unvatted code - especially in the open-source world - are often overlooked or underestimated. There are consequences to blindly integrating external code into a system.</p> <p>We've seen how there are escalation points allowing access to the underlying host with a super user, directly and indirectly:</p> <ul> <li>If running a job directly on the build agent host </li> <li>Even if running a container job, accessing the host indirectly (i.e: via the docker socket) </li> </ul> <p>Sometimes it is hard to articulate the why should I care about this. Consider this hypothetical scenario that shows how a trusted tool (CLI tool Checkov) being used in a pipeline with an unexpected extension of capability (command line argument/switch) that carries a trojanised payload. </p> <p>That is available here -&gt; From open source package to credential exfiltration from your build environment. It walks through the steps:</p> <ol> <li>Creating a malicious Open Source \"extra check\" for Checkov </li> <li>(Assumes the user decides to use that extra check functionality as there is a GitHub page that looks good enough)</li> <li>When the clitool is executed with the trojanised payload, it will silently attempt to exploit the host</li> </ol> <p>Exploit shows:</p> <ul> <li>Pulling a 'malicious' packages from the outside</li> <li>Dumping environment variables (including the build service account 'temporary' access token)</li> <li>Zipping the working directory of the build agent and exfiltrating it (includes logs, built artifacts of every pipeline, from any project, that has being executed)</li> <li>Replacing the binary that controls the Azure DevOps build agent service with one that skips the secret redaction for future pipelines executed</li> </ul> <p>Mental Note 4: When executing any binary, trusted or untrusted, there is a risk you are running unintended code which will inherit the access your pipeline has</p>"},{"location":"blog/cicd/execution_agents/#sharing-templates-a-github-actions-tale","title":"Sharing Templates (a GitHub Actions tale)","text":"<p>To exemplify this is really not just an Azure Devops thing, consider the same concepts in GitHub:</p> <ul> <li>ADO Build Agents ~= GitHub Runners (CI/CD build servers)</li> <li>ADO Pipelines ~= Workflows (pipelines)</li> <li>ADO Jobs/Steps ~= Jobs/Actions (package of work scheduled in a runner)</li> <li>ADO Extensions ~= Marketplace GitHub Actions (ways to extend pipeline capabilities with ready made, pre-defined tasks)</li> </ul> <p>GitHub Action templates are frequently shared (more frequent than with Azure DevOps pipeline templates or even Azure DevOps extensions, in my experience). This may be a by-product of the community around it and their simplicity. These actions and capability extensions should always be treated as untrusted unless proven otherwise (it's just more code written by someone else).</p> <p>Mental Note 5: There are many CI/CD Platforms enabling good users. Striving for more productive and usable workflows sometimes conflicts with the security of the environment and what gets built there</p>"},{"location":"blog/cicd/execution_agents/#what-if-i-use-microsoft-hosted-agents","title":"What if I use Microsoft-hosted agents?","text":"<p>I don't dislike Microsoft-hosted agents. There are use cases for them. And I have gone as fair as saying \"Microsoft-hosted build agents are likely more 'secure' than inadequately managed self-hosted agents\".</p> <p>It's hard to manage build agents in a balanced (adequate) way - secure, cost-efficient and performant way.</p> <p>Reasons to like Microsoft-Hosted:</p> <ul> <li>No management required (managed by MS)</li> <li>Each job runs on a fresh VM that gets destroyed by the end (harder to cross contaminate)</li> <li>Sits outside your infrastructure, reducing the likelihood of persistence/lateral movement</li> <li>Work great as a sandbox for untrusted code workflows </li> </ul> <p>Reasons not to like them:</p> <ul> <li>Significant DLP risk, and, because enterprise tooling (DLP/EDR) is not deployed, low or minimal observability over agents</li> <li>May not fit all use cases (specific builds, private network access to internal networks)</li> <li>Enterprise workflows may exceed MS license limits</li> <li>Inability to heavily monitor the agents</li> </ul> <p>You may be able to bypass any DLP/networking controls by leveraging a combination of self-hosted agents + build artifacts + Microsoft build agents, in this order. </p> <p>Example: Use a self-hosted build agent to access a restricted resource in your environment with a pipeline execution, publish artifacts with data you were able to extract, and then use a Microsoft-hosted build agent to exfiltrate those pipeline artifacts.</p>"},{"location":"blog/cicd/execution_agents/#impact","title":"Impact","text":"<p>Depending on how the build agents are shared, their lifecycle and network restrictions applied, we can think of a few stories. Examples with some knowledge of the environment:</p> <ul> <li>I want to exfiltrate data/intellectual property from my enterprise laptop. Email/endpoint DLP controls are in place and actions are easily tied back to me. So I use the enterprise CI/CD Platform to schedule a workflow on build server to exfiltrate (all/any) data.</li> <li>I want to have a cryptomining agent running in an build server. I know there are specific detection mechanisms for this, so I use the enterprise CI/CD Platform to schedule a workflow on an build server that (1) switches off security tooling, (2) downloads the cryptomining payload and (3) launches the mining client. The process is running in the background, and it persists even after the workflow is finished.</li> <li>Same as above, but I also know there is a policy to discard the agent after each workflow is run, so I intentionally make the workflow take several hours so that the cryptomining process stays active.</li> </ul> <p>If I use any of the initial access vectors (compromised account, malicious insider, open source compromise), I could also:</p> <ul> <li>Establish persistence on CI/CD build servers by spawning a privileged container on the host or initiating a reverse shell connection to my Command &amp; Control (C2) server. I use this to continuously monitor and exfiltrate data and credentials from the CI/CD build server.</li> <li>Establish persistence on CI/CD build servers by modifying the DLL that defines the CI/CD build server service so that the trusted process is running (as expected), but it has been altered to exfiltrate data and credentials anytime a workflow is scheduled there.</li> <li>I turn off or tamper with security tooling that would allow detection, response and investigations.</li> </ul> <p>There are others. It's up to the imagination. Things that make it worse:</p> <ul> <li>Unconstrained internet access</li> <li>Not segregating production from non production</li> <li>Long lived build agents</li> </ul>"},{"location":"blog/cicd/execution_agents/#conclusion","title":"Conclusion","text":"<p>Summary of mental notes:</p> <ol> <li>Whether using private, cloud-hosted or SaaS-managed machines, there is no magic, the steps of a CI/CD workflow must be executed in some piece of compute (build servers)</li> <li>Access to the host is easily obtained (directly or indirectly)</li> <li>Likely the identity used in the workflows has superuser privileges in the build server</li> <li>When executing any binary, trusted or untrusted, there is a risk you are running unintended code which will inherit the access your pipeline has</li> <li>There are many CI/CD Platforms enabling good users. Striving for more productive and usable workflows sometimes conflicts with the security of the environment and what gets built there</li> </ol> <p>In many ways, I feel many platforms already 'got got' (to quote Security Weekly News' Doug White). If I was a hacker, I'd be putting my eggs in this basket. </p> <p>If you have not 'gotten got' until today, what are the chances you'll 'get got' tomorrow? Well, I couldn't find a reason for this not to be in use today, but I could be missing something. </p> Key Consideration Details Shared across teams x target environments Sharing agents is tricky - Granularity usually means more operational/management effort. But, coarse level of segregation is better than none (production/non-production; trusted/non-trusted) Reused across pipeline runs Agents reused across runs may retain sensitive data or artifacts from previous executions. Self-hosted are recommended not only but also for performance associated with caching. Cleaning agents may reduce the appetite for self-hosted Network access control Access to package registries and target environments is required. Are these restricted to be accessed from private networks?"},{"location":"blog/cicd/protected_resources/","title":"CI/CD Resources and Pipelines","text":"<p>CI/CD pipelines are everywhere, building and deploying software continuously. Thousands of pipelines run daily, each executing tasks like code build, test, and deployment. Automation is key, often paired with guardrails to ensure quality and security.</p>"},{"location":"blog/cicd/protected_resources/#tldr-access-is-everything","title":"TLDR: Access is Everything","text":"<p>A different perspective on CI/CD security: Shifting from \"ensuring every pipeline is secure\" to \"ensuring the access to privileged resources is done under the right conditions\". </p> <ul> <li>Secure access to high-impact CI/CD resources (credentials, repos, build servers)</li> <li>Apply conditions/gates protecting resources you really care about</li> <li>Be outcome-focused: Set expectations based on risk profile</li> <li>Manage CI/CD resource lifecycle (CRUD)</li> <li>Assess the CI/CD platform as a whole, not per pipeline</li> </ul>"},{"location":"blog/cicd/protected_resources/#cicd-pipeline-basics","title":"CI/CD Pipeline Basics","text":"<p>A pipeline is a structured set of instructions that, when followed correctly, produce a desired outcome. Pipelines help ensure instructions are executed consistently and reliably, with fewer surprises.</p> <pre><code>RUN IN compute \nRUN AS user \nGET SOURCE repo\nGET VARIABLES variable_store\nDO task1\nDO task2\nDO task3\nOUTPUT logs\n</code></pre> <p>Without the resources around it, a pipeline is just a recipe in a closed recipe book - a process waiting to be executed. Ensuring the resources are accessible under the right conditions, at the time of execution, is essential.</p> <p>Semantics change across vendors, we'll focus on Azure Pipelines (concepts generally map to others).</p>"},{"location":"blog/cicd/protected_resources/#resources","title":"Resources","text":"<p>In Azure DevOps land, CI/CD resources can take multiple forms. We can grasp how they may grant the pipelines accessing them privileges on targets.</p> <ul> <li>Agent Pools: Pipelines use agent pools to execute tasks. Sometimes agents' identities, are leveraged to access specific targets.</li> <li>Service Connections: Pipelines can authenticate and interact with external targets via service connections. They represent the access to a target/action.</li> <li>Variable Groups: These may hold shared values and sensitive information allowing pipelines access to targets.</li> <li>Secure Files: Traditionally, store sensitive files such as signing certificates.</li> <li>Repositories: Where source code is stored. At times, changes to specific repositories/branches may trigger deployment actions targeting live environments.</li> <li>Environments: Collections of resources (abstraction). Usually, production/non-production is segregated.</li> </ul> <p>Anytime a resource is mentioned in this document, it can be any of the above. </p> <p>A common theme: Everything is about access.</p>"},{"location":"blog/cicd/protected_resources/#guardrails","title":"Guardrails","text":"<p>Guardrails give autonomy and confidence to developers executing engineering and development activities, while knowing if they are stepping outside what is deemed acceptable by the organisation. Often embedded in the CI/CD pipeline workflow, they catch non-optimal configurations at an early stage, ensuring cost-effective remediations. </p> <p>Guardrails are (semi) automated checks/controls designed to implement best practices, security standards, and compliance requirements. Guardrails help maintain development excellence while moving at an agile pace.</p>"},{"location":"blog/cicd/protected_resources/#privileged-resources","title":"Privileged Resources","text":"<p>Resources accessed at a given time, in a given context, are, ultimately, what grants privileges to a pipeline.</p> <p>Defining the appropriate operational and security conditions for accessing a protected resource can be challenging. It's easy to disturb the balance between controls and development. This is, however, the first step into enabling the definition of a 'what good looks like' baseline. Metrics and KPIs can be derived from it. </p> <p>An example: To deploy a binary to a production repository, it must go through the vulnerability scanning process and be signed with an organisational key.</p>"},{"location":"blog/cicd/protected_resources/#hardening-resources-access","title":"Hardening Resources Access","text":"<p>Assuming there is a defined set of operational and security conditions the pipeline must meet to access a specific resource, how can that resource be protected from misuse?</p> <p>Commonly used quality gates might provide a false sense of operational and security excellence.</p> <p></p>"},{"location":"blog/cicd/protected_resources/#overreliance-on-weak-controls","title":"Overreliance on Weak Controls","text":"<p>Native pipeline-level controls, such as the initial access validation step, are sometimes relied upon to gate access into privileged resources. Is this good enough? Once a pipeline is allowed to access a resource (build agent, source code repository, service connection), it will persist that access indefinitely. </p>"},{"location":"blog/cicd/protected_resources/#azure-pipelines-structure","title":"Azure Pipelines Structure","text":"<p>To understand why this is the case, we need to look at how Azure Pipelines are structured:</p> <ul> <li>Pipeline Definitions: Define the stage/job/step sequence for the CI/CD process and declare the resources it needs access to</li> <li>Pipeline Run: The execution of a pipeline, as defined by the pipeline definition</li> </ul> <p>While there are many valid reasons for updating pipeline definitions, the key point is, that once a pipeline build definition is granted access (following the initial resource access prompt), this access persists unless explicitly revoked - Regardless of any subsequent changes to the pipeline definition.</p> Make it real <p>1. Initial Pipeline Setup: \u00a0 Imagine a pipeline is created and granted specific access, such as to a sensitive build agent, source code repository, or a service connection with elevated privileges (e.g., access to deployment credentials or sensitive APIs). This access is granted based on the pipeline's initial configuration.</p> <p>2. Persistent Access: \u00a0 Once this pipeline definition has been authorised, it retains that access for all future pipeline runs, regardless of changes made to the definition later on. This means that after the first time it's authorised, Azure DevOps does not automatically revalidate permissions if the pipeline configuration is updated.</p> <p>3. Modifying the Pipeline: \u00a0 If you have access to update the pipeline definition (even as a lower-privileged user), you can modify the pipeline in a way that leverages the persistent access it has to the sensitive resources. For example, you can:  - Inject malicious or unauthorised tasks into the pipeline that use the existing service connection to exfiltrate sensitive data (e.g., source code, secrets, or deployment credentials). - Alter the pipeline to deploy malicious code or access restricted environments through the service connection. - Use the authorised build agent to execute privileged commands that were not part of the original pipeline's intent. </p> <p>4. Executing a Pipeline Run: \u00a0 After making these changes to the pipeline definition, you can trigger a new pipeline run. Since the persistent access to sensitive resources is still valid, the updated pipeline will be executed with elevated privileges.</p> <p>5. Stealth and Persistence: \u00a0 Because the access controls are applied at the pipeline definition level and are not re-validated with every change, this method allows you to continue executing malicious actions without triggering immediate alerts or permission checks. The pipeline may look legitimate, but it can be performing unauthorised actions in the background.</p> <p>See Weaponizing Open Source for a hypothetical scenario.</p>"},{"location":"blog/cicd/protected_resources/#but-i-use-pipeline-approvals","title":"But I Use Pipeline Approvals","text":"<p>A common scenario in CI/CD is the deployment of Terraform-declared resources into a target environment. The plan -&gt; manual approval -&gt; apply pattern allows users to do a sanity check before deploying to target environments (e.g.: production). It even allows the definition of a restricted Approvers group.</p> <p>The suitability of this control depends on what you are trying to achieve. Can I skip the approval stage? By editing the pipeline definition file you can bypass guardrails, approval steps, and even inject malicious code - There are many ways for arbitrary code to find its way to execution in your run.</p> <p>How easy is it to bypass that approval step? How stealthy can a user/attacker be? How much are you relying on this standalone approval step?</p> <p>Note: There are legitimate operational reasons, with no malicious intent, users may feel the need to bypass checks.</p>"},{"location":"blog/cicd/protected_resources/#but-i-use-branching-strategies","title":"But I Use Branching Strategies","text":"<p>That's good, however, what are you trying to achieve? Do these branching strategies alone help you get to the desired outcome? Consider whether you can create a branch from the main branch, modify the pipeline and scripts, and trigger a run with the same level of access.</p> <p>The assumption resources can only be accessed from the main branch is not always true, especially by default. You can create a branch, make changes, and execute it with the same privileges, which potentially undermines the purpose of your branching strategy.</p> <p>As Microsoft puts it: Consider prioritizing security in critical areas while accepting some trade-offs for convenience in other aspects.</p>"},{"location":"blog/cicd/protected_resources/#protected-resources","title":"Protected Resources","text":"<p>I like to look at protected resources as a mindset. Why try to secure every single pipeline when the primary concern is the resources being accessed? Ensuring that thousands of pipeline executions align with your security expectations can be overly complex and burdensome to manage. Instead, consider securing access to privileged CI/CD resources (which those pipelines can access).</p> <p>It shifts the thinking from \"ensuring every pipeline is secure\" to \"ensuring every access to privileged resources is done under the right conditions\".</p> <p>An Azure DevOps-native way to do this is through approvals and checks. This allows you to add specific actions to be triggered anytime the deemed protected resource is accessed (from manual approvals to business hours checks, required template use, and even invoking REST APIs).</p> <p>Whether you are looking for manual approval or looking to ensure specific tools are being used within your pipelines, you define the conditions that must be met for resource access to be granted.</p> <p>An additional benefit of having a protected CI/CD resource-focused approach is, it encourages a least privileged approach where there is proper segregation of privileges (otherwise, you would constantly need to apply the highest level of scrutiny to run any pipeline). Arguably, it also increases the management effort. \"Identity is easy!\", said no one, ever.</p>"},{"location":"blog/cicd/protected_resources/#disclaimers","title":"Disclaimers","text":""},{"location":"blog/cicd/protected_resources/#this-is-not-new","title":"This is not new!","text":"<p>Yet I don't see heavy adoption of a protected-resource-focused approach. I wonder why that is?</p> <p>Microsoft published this table where almost all roads lead to \"Protect resource X with checks and pipeline permissions\"</p> <p>Sometimes what we uncover about the systems we work with is simply a case of RTDM (Read The Damn Manual). Refer to Microsoft's guide on protected resources for a clearer understanding.</p>"},{"location":"blog/cicd/protected_resources/#this-is-not-just-about-security","title":"This is not just about security!","text":"<p>Protected resources are not solely about security - they are equally critical for ensuring operational excellence, sustainability, maintenance, and high-quality development processes. Balancing these factors is key to maintaining a resilient and efficient system.</p>"},{"location":"blog/cicd/protected_resources/#this-is-also-not-a-silver-bullet","title":"This is also NOT a silver bullet!","text":"<p>The elephant in the room is that CI/CD resources are merely a means for your pipelines to access a target. Issues around access control outside the CI/CD platform are not addressed by the mechanisms we discussed.</p>"},{"location":"blog/cicd/protected_resources/#next-challenge","title":"Next Challenge","text":"<p>Thousands of pipelines run daily, each executing tasks like code build, test, and deployment. We have all these tools and technologies that help us adhere to the best practices (guardrailing activities).</p> <p>How can we ensure all right conditions we've defined are met, even when native approvals and checks are not available?</p>"},{"location":"blog/cicd/protected_resources/#conclusion","title":"Conclusion","text":"<p>I would focus on protecting access to service connections, variable groups and agent pools. However, if and how you leverage this piece of information is highly dependent on your environment and strategy.</p> <p>This is the first of several upcoming research findings on CI/CD, with more insights and analysis. Stay tuned for deeper dives!</p>"},{"location":"blog/cicd/protected_resources/#references","title":"References","text":"<ul> <li>OWASP's Top 10 CI/CD Security Risks</li> <li>Microsoft's Azure Pipelines Security Resources</li> <li>Cloud Security Alliance: DevSecOps Pillars</li> <li>Daniel Abrahamberg's Enterprise Development: Guidelines, Guardrails, and Golden Paths</li> </ul>"},{"location":"blog/technical/py_git_revshell/","title":"Weaponizing Open Source","text":"<p>Often, the risks and complexities of running unvetted code - especially in the open-source world - are overlooked or underestimated. This research highlights the potential consequences of blindly integrating external code into your system, emphasizing the importance of Supply Chain Security.</p> <p>Scenario: You have a goal. You need to ensure the resources in your development pipelines are being configured according to your policy. You find a GitHub project that does exactly what you need. This project is a fork of the widely used Checkov project, which you already use anyway, but adds some useful custom checks.</p> <p>We explore how the open source tool Checkov, along with Python and Git, can be combined to exfiltrate data covertly. Particularly relevant after the discovery of the XZ Utils backdoor. </p> <p>Disclaimer</p> <p>Open source is incredible. The contributions from open source developers, who voluntarily maintain widely used projects, provide immense benefits to the community. This post is not intended to detract from the value of their work, but rather to emphasise the importance of understanding the potential risks involved when integrating external code into your system.</p>"},{"location":"blog/technical/py_git_revshell/#checkov-101","title":"Checkov 101","text":"<p>\"Checkov is a static code analysis tool for scanning infrastructure as code (IaC) files for misconfigurations that may lead to security or compliance problems. Checkov includes more than 750 predefined policies to check for common misconfiguration issues. Checkov also supports the creation and contribution of custom policies.\"</p> <p>Checkov is often integrated into build pipelines (in a CI/CD system) and can also be run locally. It is packed as a command line tool that runs a series of checks against the resources targeted by the scan. Example:</p> <p>Focusing on what happens when we are specifying extra custom checks in Checkov, we can either:</p> <ol> <li>Specify a directory with the extra checks' code:</li> </ol> <pre><code> checkov --directory *path/to/resources* --external-checks-dir *path/to/custom/checks*\n</code></pre> <ol> <li>Specify a Git repo that contains the extra checks' code.</li> </ol> <pre><code>checkov --directory *path/to/resources* --external-checks-git *customchecks_giturl*\n</code></pre> <p>Whichever the way chosen, Checkov checks are really just running code, and Python-based custom checks, are really just running custom Python code.</p> <p>Going back to the original scenario. You are almost achieving your goal of ensuring that all your resources are configured according to custom checks, and this is enabled by the (free!) custom extra checks you found. Great win. What can go wrong?</p>"},{"location":"blog/technical/py_git_revshell/#data-exfiltration-with-python-and-git","title":"Data Exfiltration with Python and Git","text":"<p>When running Checkov checks (custom or not), you are running code, in your environment, with all the permissions the user runinng it has. This is a great foothold into your environment.</p> <ul> <li>We have (almost arbitrary) command execution with Python</li> <li>We can leverage tools such as Git to in/exfiltrate data</li> </ul> <p>To illustrate this, we have exploit code for:</p> <ul> <li> <p>A proof of concept CKV_COOL_MALI_CHECK that creates a user, generates files, runs bash commands, and installs an OpenSSH server on the machine running Checkov. This allows remote connections using the new user. While this requires sudo privileges and firewall rules might complicate access, the potential for misuse is significant, especially regarding filesystem integrity.</p> </li> <li> <p>A proof of concept CKV_COOL_MALI_2_CHECK to silently exfiltrate data with Python and Git:</p> </li> <li> <p>Clones a remote repository (git clone)</p> </li> <li>Creates a new branch* (git checkout)</li> <li>Creates a file with data about the system in the cloned repo directory (keep it simple for PoC, endless opportunities)</li> <li>Commits file to the .git tree (git add &amp; commit)</li> <li>Pushes to the Git repo (git push)</li> <li>Deletes traces of its activity (shutil.rmtree)</li> </ul> <p></p>"},{"location":"blog/technical/py_git_revshell/#conclusion","title":"Conclusion","text":"<p>The sky is the limit here. With or without root privileges, you can leave a mark. My advice is not to run extra checks unless the code has been reviewed and tested by yourself (you do have to do the work, the \"it's fine, open source means it's safe because everyone can check it\" solution has been proven NOT to work).</p> <p>This caution applies to any tool where the source code has not been scrutinized. It's not just Checkov, nor is it just Checkov custom checks.</p>"},{"location":"blog/technical/py_git_revshell/#actions","title":"Actions","text":"<ul> <li> <p>Reached out to Palo Alto through their security reports page.</p> </li> <li> <p>Checkov documentation has been updated.</p> </li> </ul>"},{"location":"blog/technical/py_git_revshell/#related-work-stories","title":"Related Work &amp; Stories","text":"<p>This research was triggered when developing custom policies to help with metadata guardrails for container images and CSP infrastructure (labels, tags, annotations, etc). Metadata becomes more complex than expected when you consider the semantics used by different providers, the asset types and the lack of maturity of tools in this space. A starter-for-ten is available on pac-on-rails.</p>"},{"location":"blog/technical/cicd/exec_agents/","title":"From open source package to credential exfiltration from your build environment","text":"<p>The problem statement and state of the art when it comes to the security of build environments is introduced in CI/CD Build Servers. This is a practical walkthrough of a scenario using a trusted open source package to have access to the build environment and then exploit it.</p> <p>Sometimes it is hard to articulate the why should I care about this. This is a report on a hypothetical scenario where we have a trojanised open-source tool integrated in our build pipelines, and how that can be leveraged to exfiltrate data/credentials, persistence, lateral movement and Denial-of-Service/Wallet attacks. </p> <p>It consists of:</p> <ol> <li>Creating a malicious Open Source \"extra check\" for Checkov </li> <li>(Assumes the user decides to use that extra check fuctionality as there is a GitHub page that looks good enough)</li> <li>When the clitool is executed with the trojanised payload, it will silently attempt to exploit the host</li> </ol> <p>And the exploit shows this is possible:</p> <ul> <li>Pulling a 'malicious' packages from the outside</li> <li>Dumping environment variables (including the build service account 'temporary' access token)</li> <li>Zipping the working directory of the build agent and exfiltrating it (includes logs, built artefacts of every pipeline, from any project, that has being executed)</li> <li>Replacing the binary that controls the Azure DevOps build agent service with one that skips the secret redaction for future pipelines executed</li> </ul>"},{"location":"blog/technical/cicd/exec_agents/#walkthrough","title":"Walkthrough","text":"<p>Scenario: You have a goal. You need to ensure the resources in your development pipelines are being configured according to your policy. You find a GitHub project that does exactly what you need. This project is a fork of the widely used Checkov project, which you already use anyway, but adds some useful custom checks.</p> <p>This is not a new scenario (reused from an earlier article), but it's just as plausible. Alternatively, this could be done by creating any 'useful' CLI tool. I like this scenario because Checkov itself is already trusted and expected to be ran in pipelines, and all we are doing is adding an extension to its out-of-the-box functionality.</p>"},{"location":"blog/technical/cicd/exec_agents/#step-1","title":"Step #1","text":"<p>When running Checkov checks (custom or not), you are running code, in your environment, with all the permissions the user runinng it has. We will extend the malicious checks in the pac-on-rails GitHub repository to have an additional a malicious check (COOL_MALI_3) that takes advantage of the access to the underlying build agent.</p>"},{"location":"blog/technical/cicd/exec_agents/#step-2","title":"Step #2","text":"<p>Assuming the user found this project, read the README, which made it look like any other useful open source CLI tool hosted on GitHub, the user adds a simple switch to the pipeline. The user runs the pipeline and gets the desired result.</p>"},{"location":"blog/technical/cicd/exec_agents/#step-3","title":"Step #3","text":"<p>When the pipeline was run, alongside the useful functionality, the trojanised payload also executed and we are to confirm all our objectives were met.</p> <ul> <li>Pulling 'malicious' packages from the outside</li> <li>Dumping environment variables (including the build service account 'temporary' access token)</li> <li>Zipping the working directory of the build agent and exfiltrating it (includes logs, built artefacts of every pipeline, from any project, that has being executed)</li> <li>Replacing the binary that controls the Azure DevOps build agent service with one that skips the secret redaction for future pipelines executed</li> </ul> <p>More details to be added soon</p>"},{"location":"blog/technical/cicd/exec_agents/#conclusion","title":"Conclusion","text":"<p>Only do this at home :)</p>"},{"location":"blog/technical/cicd/exec_agents/#other-not-so-interesting-facts","title":"Other (not so) interesting facts","text":"<ul> <li>Running containarised jobs and tasks in pipelines makes the initialisation of the pipeline workflows take anytime from quick to an annoying amount of time depending on the availability of the images required being already in the build agent.</li> <li>The build agent access token is not invalidated after each run. A new one is generated, but the old one stays active (for some time).</li> <li>Windows Defender definetely recognised obvious payloads from Metasploit :D</li> </ul>"},{"location":"blog/technical/cicd/pipeline_influencer/","title":"Pipeline influencer","text":"<p>\ud83d\udea7 This page is under construction. Check back later!</p>"},{"location":"blog/technical/pac/pac_on_rails/","title":"pac-on-rails","text":"<p>The code, tests and research are available in the public repository pac-on-rails, feel free to have a look and tailor to your needs. In here for completeness.</p> <p>Mini project to automate Policy as Code (PaC) and guardrails to ensure adherence to tagging/labelling policies. Supports Traditional IaC Infrastructure and Container Images.</p>"},{"location":"blog/technical/pac/pac_on_rails/#policy-engine","title":"Policy Engine","text":"<ul> <li> <p>The repository holds Checkov custom policies to perform checks against container images' labelling policies. The engine ensures compliance with the specified policy (see customisation section).</p> </li> <li> <p>The repository holds Checkov custom policies to perform checks against infrastructure metadata policies (Terraform-defined). The engine ensures compliance with the specified policy (see customisation section).</p> </li> </ul>"},{"location":"blog/technical/pac/pac_on_rails/#metadata-policy-format-container-images-traditional-iac-infrastructure","title":"Metadata Policy Format (Container Images &amp; Traditional IaC Infrastructure)","text":"<p>The tagging policy is defined using JSON format, allowing for flexible customisation of metadata pairs and their validation rules.</p> <pre><code>{\n    \"maintainer\": {\n        \"allowed_values\": \".*\",\n        \"version\": \"1.0\",\n        \"description\": \"A sample metadata pair - Any value accepted\"\n    },\n    \"maintainer_specific\": {\n        \"allowed_values\": \"^[\\w\\.-]+@[a-zA-Z0-9-]+\\.[a-zA-Z]{2,}$\",\n        \"version\": \"1.0\",\n        \"description\": \"A sample metadata pair - Specific regex\"\n    }\n}\n</code></pre>"},{"location":"blog/technical/pac/pac_on_rails/#extendable-csp-specific-configuration-traditional-iac-infrastructure","title":"Extendable CSP-specific Configuration (Traditional IaC Infrastructure)","text":"<p>Each cloud provider has its own configuration section within the engine. The JSON file lists specific configurations to take, according to the CSP. This is here, not only, but also because CSPs use different terms to refer to the same thing.</p> <p>This policy is currently applicable to Terraform resources. As you will find out, different providers, and even different resources within the same provider, use different attributes and attribute \"paths\" to apply metadata at different scopes. </p> <pre><code>{\n  \"azure\": {\n      \"keys\": [\n          \"arm\",\n          \"az\",\n          \"azure\",\n          \"azurerm\"\n      ],\n      \"description\": \"Resources to check for metadata pairs (Azure). To override this, modify this file, leaving it in the working directory checkov runs from.\",\n      \"supported_types\": [\n          {\n              \"name\": \"azurerm_kubernetes_cluster\",\n              \"tag_paths\": [\n                  {\n                      \"path\": \"\",\n                      \"attributes\": {\n                          \"one_of\": [],\n                          \"required\": [\n                              {\n                                  \"name\": \"tags\",\n                                  \"cloud_native\": false\n                              }\n                          ]\n                      }\n                  },\n                  {\n                      \"path\": \"default_node_pool\",\n                      \"attributes\": {\n                          \"one_of\": [],\n                          \"required\": [\n                              {\n                                  \"name\": \"tags\",\n                                  \"cloud_native\": false\n                              }\n                          ]\n                      }\n                  }\n              ]\n          }\n      ]\n  }\n}\n</code></pre> <ul> <li><code>csp</code>: Configuration related to the CSP resources. Contains details on how to validate tags for that CSP infrastructure.</li> <li> <p>Example Values: <code>azure</code>, <code>google</code>, <code>aws</code></p> </li> <li> <p><code>keys</code>: List of identifiers used to recognize CSP-related resources. These keywords help in identifying and applying the tagging policy to the correct resource types.</p> </li> <li> <p>Example Values: <code>[\"arm\", \"az\", \"azure\", \"azurerm\"]</code> or <code>[\"gcp\", \"google\", \"googlecloud\"]</code></p> </li> <li> <p><code>supported_types</code>: List of CSP resource types targeted by the tagging policy. Each type includes specific tag paths and requirements.</p> </li> <li>Example Values:<ul> <li><code>name</code>: The name of the resource type (e.g., <code>azurerm_kubernetes_cluster</code>).</li> <li><code>tag_paths</code>: List of paths within the resource where tags are validated.</li> <li><code>path</code>: Optional sub path eithin the resource configuration, leads to the tags attribute (e.g., <code>default_node_pool</code>).</li> <li><code>attributes</code>:<ul> <li><code>one_of</code>: At least one of these must be present (e.g., <code>tags</code>).</li> <li><code>required</code>: All these attributes must be present (e.g., <code>tags</code>).</li> </ul> </li> </ul> </li> </ul>"},{"location":"blog/technical/pac/pac_on_rails/#customise","title":"Customise","text":"<p>Customise with cloud_specific_configurations.json and policy.json as needed. Else, it will use a pre-def set of values. To customise a file, it needs to be in the dir where checkov runs from (see future work).</p>"},{"location":"blog/technical/pac/pac_on_rails/#security-advisory-before-running","title":"Security Advisory - Before Running","text":"<p>To use extra checks with Checkov CLI tool, we can:</p> <ol> <li>Specifying a directory with the extra checks' code, or</li> <li>Specifying a Git repo that contains the extra checks' code.</li> </ol> <p>I'd recommend (1). And if you ask me about number (2), I'd ensure the checks are from a private and/or very well scrutinised git repo. </p> <p>Checkov checks are really just running extra code:</p> <ul> <li>With custom Python checks, it's running custom Python scripts. </li> <li>With custom Python checks from a remote repo, it's running (potentially dangerous) custom Python scripts from a remote repo. </li> </ul> <p>Sky is the limit here. With or without root privileges, you can leave a mark. My advice is not to run extra checks unless the code has been reviewed and tested by yourself (you do have to do the work, the \"it's fine, open source means it's safe because everyone can check it\" solution has been proven NOT to work). This is true with any (especially open source) tool. </p> <p>That said, I feel the official Checkov documentation and GitHub pages could be clearer. The only place I could find that lightly touches on this is a small paragraph on best practices in the GitHub page (https://github.com/bridgecrewio/checkov?tab=readme-ov-file#configuration-using-a-config-file).</p> <p>Update: Reached out to Palo Alto through their security reports page. Checkov documentation has been updated.</p>"},{"location":"blog/technical/pac/pac_on_rails/#malicious-check-1-user-ssh","title":"Malicious Check 1 - User + SSH","text":"<p>I done a simple PoC that creates a user, creates a files and runs bash commands. It also installs an openssh server in the machine running Checkov which we can connect to it, using the created user. Installs and user creation requires sudo priviledges, and connections may be harder with FW rules in place, but possibilities are endless, and I have seen loads of scary attacks on workloads. Particularly scary on the integrity side, as we can mess with the filesystem.</p> <pre><code>1. Try to SSH (not running, so I have connection refused)\n2. Run malicious check from remote git repo\n3. SSH with the newly created user\n</code></pre>"},{"location":"blog/technical/pac/pac_on_rails/#malicious-check-2-infiltrate-exfiltrate-data","title":"Malicious Check 2 - Infiltrate + Exfiltrate data","text":"<p>Take a step back, what would an attacker want?</p> <ul> <li>We have (almost arbitrary) command execution </li> <li>I guess we need a way to infiltrate/exfiltrate data!</li> </ul> <p>Queue CKV_COOL_MALI_2_CHECK. This check silently:</p> <ol> <li>Clones a remote repository (git clone)</li> <li>Creates a new branch* (git checkout)</li> <li>Creates a file with data about the system in the cloned repo directory (keep it simple for PoC, endless opportunities)</li> <li>Commits file to the .git tree (git add &amp; commit)</li> <li>Pushes to the Git repo (git push)</li> <li>Deletes traces of its activity (shutil.rmtree)</li> </ol> <p>*(because our attacker adheres to best practices and does not push to main ;) )</p> <p>I wanted some kind of reverse shell, but was being slowed by some connection refused errors.</p> <p></p>"},{"location":"blog/technical/pac/pac_on_rails/#run","title":"Run","text":""},{"location":"blog/technical/pac/pac_on_rails/#container-images","title":"Container Images","text":"<p>checkov -d containerfile_dir/ --external-checks-dir checks/metadata/images --framework dockerfile -c CKV_DOCKER_LABEL_CHECK</p>"},{"location":"blog/technical/pac/pac_on_rails/#traditional-iac-infrastructure","title":"Traditional IaC Infrastructure","text":"<p>checkov -d terraform_dir/ --external-checks-dir checks/metadata/infrastructure -c CKV_TF_METADATA_CHECK </p>"},{"location":"blog/technical/pac/pac_on_rails/#future-work","title":"Future Work","text":"<ul> <li>Combine Checks: Explore combining Cloud-Native (CN) and traditional IaC checks into a unified metadata validation approach.</li> <li>Customize Input Paths: Customize the path to input files (policy.json, cloud_specific_configurations.json) for higher flexibility.</li> </ul>"},{"location":"blog/technical/pac/pac_on_rails/#feedback-contact","title":"Feedback &amp; Contact","text":"<p>For questions, feedback, bug reports, please use one of the below:</p> <ol> <li> <p>GitHub Issues: Report bugs or request features on GitHub Issues page.</p> </li> <li> <p>Email: For general feedback, you can email at ytimyno@gmail.com.</p> </li> </ol>"}]}